\section{Inventing the Factorial Function}
\begin{exercise}
For \(n \in N\), let
\[N\# = n + (n-1) + (n-1) + \cdots + 2 + 1\]
\enum{
\item Without looking ahead, decide if there is a natural way to define \(0\#\). How about \((-2)\#\)? Conjecture a reasonable value for \(\frac{7}{2}\#\).
\item Now prove \(n\# = \frac{1}{2}n (n+1)\) for all \(n \in \mathbf{N}\), and revisit part (a).
}
\end{exercise}
\begin{solution}
\enum{
\item Noting that \(N\# = (N-1)\# + N\) and \(1\# = 1\), we could have \(0\# = 0\), \(-1\# = 0\), and \(-2\# = 1\). \(7/2\#\) could just be defined to be the result from linearly interpolating between \(3\# = 6\) and \(4\# = 10\) to get \(8\).
\item This is obviously true for \(n = 1\), and
\[(n+1)\# = n+1+n\# = n+1 + \frac{1}{2}n(n+1) = (n+1)\left(1 + \frac{n}{2}\right) = (n+1)\left(\frac{n+2}{2}\right)\]
which proves the formula by induction.
}
\end{solution}
\begin{exercise}
Verify that the series converges absolutely for all \(x \in \mathbf{R}\), that \(E(x)\) is differentiable on \(\mathbf{R}\), and \(E'(x) = E(x)\).
\end{exercise}
\begin{solution}
Note that
\[\sum^\infty_{n=0} \abs{\frac{x^n}{n!}} = E(\abs{x})\]
so we only need to show the series converges for \(x \geq 0\).

Fix \(x \geq 0\) and let \(N > x\) for \(N \in \mathbf{N}\). We have
\[E(x) = \sum^{2N}_{n=0} \frac{x^n}{n!} + \sum_{n=2N+1}^\infty \frac{x^n}{n!} \leq K + \sum_{n=2N+1}^\infty \frac{N^n}{N!(N+1)^n} = K + \frac{1}{N!}\sum_{n=2N+1}^\infty \left(\frac{N}{N+1}\right)^n\]
for some finite constant \(K\). The infinite series left over is a geometric series which converges.

Term-by-term differentiation is safe to apply on power series which converge (Theorem 6.5.6), and it's clear that \(E'(x) = E(x)\) when applying termwise differentiation.
\end{solution}

\begin{exercise}
\enum{
\item Use the results of Exercise 2.8.7 and the binomial formula to show that \(E(x+y) = E(x)E(y)\) for all \(x,y\in \mathbf{R}\).
\item Show that \(E(0) = 1\) , \(E(-x) = 1/E(x)\), and \(E(x) > 0\) for all \(x \in \mathbf{R}\).
}
\end{exercise}
\begin{solution}
\enum{
\item
\[\begin{aligned}
    E(x+y) &= \sum^\infty_{n=0} \frac{(x+y)^n}{n!} = \sum^\infty_{n=0} \frac{1}{n!} \sum^n_{i=0} \frac{n!}{i!(n-i)!} x^i y^{n-i} = \sum^\infty_{n=0} \sum^n_{i=0} \left(\frac{x^i}{i!}\right) \left(\frac{y^{n-i}}{(n-i)!}\right) \\
    &= \sum^\infty_{i=0} \sum^\infty_{j=0} \left(\frac{x^i}{i!}\right) \left(\frac{y^j}{j!}\right) = \left(\sum^\infty_{i=0} \frac{x^i}{i!}\right)\left(\sum^\infty_{i=0} \frac{y^i}{i!}\right) = E(x) E(y)
\end{aligned}
    \]
\item For \(E(0)\), all terms for \(n \geq 1\) become 0, so \(E(0) = 1\). We showed somewhat informally in Exercise 6.6.5(c) that \(E(-x) E(x) = 1\) by collecting common terms. However, Exercise 2.8.7 lets us conclude that \(E(-x)E(x)\) does in fact equal \(\sum^\infty_{n=0} d_n\) where \(d_n\) is defined as in Exercise 6.6.5(c). Finally, it's clear that \(E(x) > 0\)  for \(x \geq 0\) simply because all terms are positive; then \(E(-x) = 1/E(x) > 0\) and so \(E(x) > 0\) for \(x < 0\) as well.
}
\end{solution}

\begin{exercise}
Define \(e = E(1)\). Show \(E(n) = e^n\) and \(E(m/n) = \left(\sqrt[n]{e}\right)^m\) for all \(m,n \in \mathbf{Z}\).
\end{exercise}
\begin{solution}
We have for \(n \geq 1\) that
\[E(n) = E\left(\sum^n_{i=1} 1\right) = \prod^n_{i=1} E(1) = e^n \]
We also have for \(n=0\), \(E(0) = 1 = e^0\), by the standard definition of \(a^n\) for any \(a \in \mathbf{R}\). Finally for \(n > 0\), \(e^{-n} = 1/e^n = 1/E(n) = E(-n)\), so \(e^n = E(n)\) for all \(n \in \mathbf{Z}\).

By definition \(\sqrt[n]{e}\) is the unique positive number which satisfies \((\sqrt[n]{e})^n = e\). \(E(1/n)\) satisfies this equality, since
\[E(1) = E\left(\sum^n_{i=1} \frac{1}{n}\right)= \prod^n_{i=1} E(1/n) \]
so \(\sqrt[n]{e} = E(1/n)\). Finally
\[E(m/n) = E\left(\sum^m_{i=1} \frac{1}{n}\right) = \prod^m_{i=1}E(1/n) = \left(\sqrt[n]{e}\right)^m\]
\end{solution}

\begin{exercise}
Show \(\lim_{x \to \infty} x^n e^{-x} = 0\) for all \(n = 0,1, 2, \dots\).
\end{exercise}
\begin{solution}
Note that for a fixed \(n\), \(\forall K\in \mathbf{R}\),we can find \(N > 0\) so that whenever \(x \geq N\), \(e^x x^{-n} > K\). We do this by noting
\[\frac{E(x)}{x^n} > \frac{x^{n+1}}{(n+1)! x^n} = \frac{x}{(n+1)!}\]
and setting \(N = K(n+1)! \).

Now, let \(\epsilon > 0\), and find \(M\) so that \(x \geq M\) implies \(e^x x^{-n} > 1/\epsilon\). Then
\[x^n e^{-x} = \frac{1}{x^{-n}e^x} < \epsilon\]
as desired.
\end{solution}

\begin{exercise}
\enum{
\item Explain why we know \(e^x\) has an inverse function---let's call it \(\log x\)---defined on the strictly positive real numbers and satisfying
\enumr{
    \item \(\log(e^y) = y\) for all \(y \in \mathbf{R}\) and
    \item \(e^{\log x} = x\), for all \(x > 0\).
}
\item Prove \((\log x)' = 1/x.\) (See Exercise 5.2.12.)
\item Fix \(y > 0\) and differentiate \(\log(xy)\) with respect to \(x\). Conclude that
\[\log (xy) = \log x + \log y \text{\quad for all } x,y > 0.\]
\item For \(t > 0\) and \(n \in \mathbf{N}\), \(t^n\) has the usual interpretation as \(t \cdot t \cdots t\) (n times). Show that
\[t^n = e^{n \log t} \text{\quad for all } n \in \mathbf{N}\]
}
\end{exercise}
\begin{solution}
\enum{
\item Let \(y > x\), and let \(z = y-x > 0\). Noting that \(e^z > 1\) (as can be easily verified by looking at the definition of \(E\)), \(e^y = e^x e^z > e^x\) and therefore \(e^x\) is strictly increasing; therefore \(e^x\) does have an inverse function. In Exercise 8.4.5 we showed that \(e^x\) can achieve any value in \((0, \infty)\); hence \(\log x\) is defined for \(x > 0\). \(\log(e^y) = y\) and \(e^{\log x} = x\) stem from the definition of an inverse function.
\item For convenience of notation let \(f(x) = e^x\) and \(g(x) = f^{-1}(x) = \log x\). We have, from Exercise 5.2.12,
\[(\log x)' = g'(x) = \frac{1}{f'(g(x)} = \frac{1}{f(g(x)} = \frac{1}{e^{\log x}} = \frac{1}{x}\]
\item
\[(\log (xy))' = \frac{y}{xy} = \frac{1}{x} = (\log x)'\]
and therefore \(\log (xy) = \log x + C \) for some constant \(C\). Now
\[xy = e^{\log(xy)} = e^{\log x} e^C = x e^C \]
and therefore \(y = e^C\) and \(C = \log y\).
\item Combine \(t = e^{\log t}\) and \(\prod^n_{i=1} e^a = e^{na}\)
}
\end{solution}

\begin{exercise}
\enum{
\item Show \(t^{m/n}\) = \((\sqrt[n]{t})^m\) for all \(m, n \in \mathbf{N}\).
\item Show \(\log(t^x) = x \log t\), for all \(t > 0\) and \(x \in \mathbf{R}\).
\item Show \(t^x\) is differentiable on \(\mathbf{R}\) and find the derivative.
}
\end{exercise}
\begin{solution}
\enum{
\item The properties of \(e^x\) proved in Exercise 8.4.3 are trivially shown to be true of \(t^x\) as well. Then the same strategy as Exercise 8.4.4 can be taken, replacing \(e^x\) with \(t^x\).
\item Noting that \(t^x > 0\), take \(\log\) of both sides of the definition of \(t^x\).
\item
\[(t^x)' = \left(e^{x \log t}\right)' = \left(e^{x \log t}\right) \log t = t^x \log t\]
}
\end{solution}

\begin{exercise}
    Inspired by the fact that \(0! = 1\) and \(1! = 1\), let \(h(x)\) satisfy
\enumr{
\item \(h(x) = 1\) \quad for all \(0 \leq x \leq 1\), and
\item \(h(x) = x h(x-1) \) \quad for all \(x \in \mathbf{R}\).
}
\enum{
    \item Find a formula for \(h(x)\) on \([1,2]\), \([2,3]\), and \([n, n+1]\) for arbitrary \(n \in \mathbf{N}\).
    \item Now do the same for \([-1, 0]\), \([-2, -1]\), and \([-n, -n+1]\).
    \item Sketch \(h\) over the domain \([-4, 4]\).
}
\end{exercise}
\begin{solution}
\enum{
\item Over \([1,2]\), \(h(x) = x h(x-1) = x\). Over \([2,3]\), \(h(x) = x h(x-1) = x(x-1)\). Over \([n, n+1]\), \(h(x) = \prod^{n-1}_{i=0} x-i\), which can readily be proven with induction.
\item Over \([-1, 0]\), \((x+1) h(x) = h(x+1) = 1 \implies h(x) = \frac{1}{x + 1}\). Over \([-2, 1]\), \((x+1) h(x) = h(x+1) = \frac{1}{(x+1)} \implies h(x) = \frac{1}{(x+1)^2}\). Over \([-n, -n+1]\), \(h(x) = (x+1)^{-n}\).
\item This is an exercise best left for your favourite graphing utility.
}
\end{solution}

\begin{exercise}
\enum{
\item Show that the improper integral \(\int_a^\infty f\) converges if and only if, for all \(\epsilon > 0\) there exists \(M > a\) such that whenever \(d > c \geq M\) it follows that
\[\abs{\int_c^d f} < \epsilon.\]
(In one direction it will be useful to consider the sequence \(a_n =  \int_a^{a+n} f\).)
\item Show that if \(0 \leq f \leq g\) and \(\int_a^\infty g\) converges then \(\int_a^\infty f\) converges.
\item Part (a) is a Cauchy criterion, and part (b) is a comparison test. State and prove an absolute convergence test for improper integrals.
}
\end{exercise}
\begin{solution}
\enum{
\item
In the forward direction, assume that \(\lim_{b \to \infty}\int_a^b f\) exists. Note that for any \(c\),
\[\int_c^\infty f = \lim_{b \to \infty} \int_a^c f + \int_c^b f - \int_a^c f = \int_c^a f + \int_a^\infty f \]
Now, choose \(M\) large enough to ensure that for \(m \geq M\),
\[\abs{\int_a^\infty f - \int_a^m f} = \abs{\int_m^\infty f} < \frac{\epsilon}{2}\]
Now if \(d > c \geq M\),
\[\abs{\int_c^d f} = \abs{\int_c^\infty f - \int_d^\infty f} \leq \abs{\int_c^\infty f} + \abs{\int_d^\infty f} < 2 \frac{\epsilon}{2} = \epsilon\]

In the reverse direction, define \(M_n\) large enough so that \(d > c \geq M_n\) implies \(\abs{\int_c^d f} < 1/n \) and satisfying \(M_n > a + n\). Define the sequence \(x_n = \int_a^{M_n} f\), and note that \((x_n)\) is a Cauchy sequence and converges to some limit \(x\). Thus for any \(\epsilon > 0\), we can find \(N_1\) so that for \(n > N_1\),
\[\abs{x - \int_a^{M_n} f} < \frac{\epsilon}{2}\]
and set \(N > \max\{2/\epsilon, N_1 \}\) so that for \(b > c =M_{N} \),
\[\abs{\int_{M_N}^b f} = \abs{\int_a^b f - \int_a^{M_N} f} < \frac{\epsilon}{2}\]
and so \(b > M_N\) implies
\[\abs{\int_a^b - x} \leq \abs{x - \int_a^{M_N} f} + \abs{\int_a^{M_N} f - \int_a^b f} < \epsilon\]
showing that \(\int_a^\infty f = x\).
\item For any \(\epsilon > 0\), from part (a) and since \(\int_a^\infty g\) converges, \(\exists M\) so that for \(d > c \geq M\),
\[\epsilon > \abs{\int_c^d g} = \int_c^d g \geq \int_c^d f = \abs{\int_c^d f}\]
implying \(\int_a^\infty f\) converges, where we have taken advantage of the fact that \(\int_c^d f \geq 0\) and \(\int_c^d g \geq 0\).

\item The test is that if \(\int_a^\infty |f|\) converges, then so does \(\int_a^\infty f\). To prove this, we use the same strategy as part (b): for any \(\epsilon > 0\), \(\exists M\) so that for \(d > c \geq M\),
\[\epsilon > \abs{\int_c^d \abs{f}} = \int_c^d \abs{f} \geq \abs{\int_c^d f}\]
implying \(\int_a^\infty f\) converges.

}
\end{solution}

\begin{exercise}
\enum{
\item Use the properties of \(e^t\) previously discussed to show
\[\int_0^\infty e^{-t} dt = 1.\]
\item Show
\[\frac{1}{\alpha} = \int_0^\infty e^{-\alpha t} dt, \text{\quad for all } \alpha > 0.\]
}
\end{exercise}
\begin{solution}
\enum{
\item The antiderivative of \(e^{-t}\) is \(-e^{-t}\), so
\[\lim_{b \to \infty} \int_0^b e^{-t} dt = \lim_{b \to \infty} = \lim_{b \to \infty} -e^{-b} + e^0 = 1\]
\item The antiderivative of \(e^{-\alpha t}\) is \(-e^{-\alpha t} / \alpha\), so
\[\lim_{b \to \infty} \int_0^b e^{-\alpha t} dt  = \lim_{b \to \infty} \frac{1}{\alpha }\left(1 -e^{-\alpha b}\right) = \frac{1}{\alpha}\]
}
\end{solution}

\begin{exercise}
\enum{
\item Evaluate \(\int_0^b t e^{-\alpha t}\) using the integration-by-parts formula from Exercise 7.5.6. The result will be an expression in \(\alpha\) and \(b\).
\item Now compute \(\int_0^\infty t e^{-\alpha t} dt \) and verify equation (4).
}
\end{exercise}
\begin{solution}
\enum{
\item
\[\int_0^b te^{-\alpha t} dt = -\frac{b e^{-\alpha b}}{\alpha} + \frac{1}{\alpha} \int_0^b e^{-\alpha t} =-\frac{b e^{-\alpha b}}{\alpha} + \frac{1}{\alpha^2} (1 - e^{-\alpha b})\]
\item \(\lim_{b \to \infty} -\frac{b e^{-\alpha b}}{\alpha} + \frac{1}{\alpha^2} (1 - e^{-\alpha b}) = \frac{1}{\alpha^2}\)
}
\end{solution}

\begin{exercise}
Assume the function \(f(x,t)\) is continuous on the rectangle \(D = \{(x,t):a \leq x \leq b,c \leq t \leq d\}\). Explain why the function
\[F(x) = \int_c^d f(x,t) dt\]
is properly defined for all \(x \in [a,b]\).
\end{exercise}
\begin{solution}
For a fixed \(x\), \(f(x,t)\) is continuous with respect to \(t\). To see this, we need to show \(\forall t_0 \in [c,d],\epsilon > 0\), we can find \(\delta\) so that \(\abs{t - t_0} < \delta \) implies \(\abs{f(x,t) - f(x, t_0)} < \epsilon\). But since
\[\abs{t - t_0} = \|(x,t) - (x, t_0) \|\]
we can just use the continuity of \(f\) over \(D\) to conclude \(f(x,t)\) is continuous with respect to \(t\), and therefore \(\int_c^d f(x,t) dt\) is properly defined.
\end{solution}

\begin{exercise}
Prove Theorem 8.4.5.
\end{exercise}
\begin{solution}
Since \(f(x,t)\) is uniformly continuous, for any \(\epsilon > 0\), we can find \(\delta\) so that \(\abs{f(x,t) - f(x_0, t)} < \frac{\epsilon}{d-c}\) whenever \(\|(x,t) - (x_0, t) \| = \abs{x - x_0} < \delta\). Then when \(\abs{x - x_0} < \delta\),
\[\abs{F(x) - F(x_0)} \leq \int_c^d \abs{f(x, t) - f(x_0, t)} dt < \int_c^d \frac{\epsilon}{d-c} dt = \epsilon \]
showing \(F(x)\) is continuous.
\end{solution}

\begin{exercise}
Finish the proof of Theorem 8.4.6.
\end{exercise}
\begin{solution}
    Note that
    \[\frac{F(z) - F(x)}{z-x} = \int_c^d \frac{f(z,t) - f(x,t)}{z-x} dt\]
so choosing \(\delta\) so that \(0 < |z-x| < \delta\) implies
\[\abs{f_x(x,t) - \frac{f(z,t) - f(x,t)}{z-x}} < \frac{\epsilon}{d-c}\]
we have
\[\abs{\frac{F(z) - F(x)}{z-x} - \int_c^d f_x(x,t)dt} = \abs{\int_c^d \frac{f(z,t) - f(x,t)}{z-x} - f_x(x,t)} < \int_c^d \abs{\frac{\epsilon}{d-c}} = \epsilon\]
as desired.
\end{solution}

\begin{exercise}
\enum{
\item Show that the improper integral \(\int_0^\infty e^{-xt} dt\) converges uniformly to \(1/x\) on the set \([1/2, \infty)\).
\item Is the convergence uniform on \(0, \infty\)?
}
\end{exercise}
\begin{solution}
\enum{
\item From our earlier work, we have that
\[\int_0^\infty e^{-xt} dt - \int_0^d e^{-xt} = \frac{e^{-xd}}{x} \leq 2e^{-d/2}\]
Notably this is not dependent on \(x\), and since \(\lim_{d \to \infty} 2e^{-d/2} = 0\), we have that \(\int_0^\infty e^{-xt} dt\) converges uniformly.
\item For any fixed \(d\), \[\lim_{x \to 0^+} \frac{e^{-xd}}{x} = \infty\]
implying that the convergence cannot be uniform on \((0, \infty)\).
}
\end{solution}

\begin{exercise}
Prove the following analogue of the Weierstrauss M-Test for improper integrals: If \(f(x,t)\) satisfies \(\abs{f(x,t)} \leq g(t)\) for all \(x \in A\) and \(\int_a^\infty g(t) dt\) converges, then \(\int_a^\infty f(x,t) dt\) converges uniformly on \(A\).
\end{exercise}
\begin{solution}
We want, \(\forall \epsilon> 0\), to find \(M > a\) so
\[\abs{\int_a^\infty f(x,t) dt - \int_a^b f(x,t) dt} <\epsilon\]
for all \(b > M\). But
\[\abs{\int_a^\infty f(x,t) dt - \int_a^b f(x,t) dt} = \abs{\int_b^\infty f(x,t) dt} \leq \int_a^\infty  \abs{f(x,t)} dt \leq \int_b^\infty g(t) dt \]
and since \(\int_a^\infty g(t)\) converges, \(\lim_{b \to \infty} \int_b^\infty g(t) dt  = 0\), which can easily be used to finish the proof.
\end{solution}

\begin{exercise}
Prove Theorem 8.4.8.
\end{exercise}
\begin{solution}
Let \(F_n(x) = \int_c^{c+n} f(x,t)dt\) and note that \(F_n(x)\) is continuous. So
\(F(x) = \lim_{n \to \infty} F_n(x)\) must also be continuous, and since \([a,b]\) is compact \(F(x)\) is also uniformly continuous over \([a,b]\).
\end{solution}

\begin{exercise}
    Prove Theorem 8.4.9.
\end{exercise}
\begin{solution}
Let \(g_n(x) = \int_c^{c+n} f(x,t) dt\). From Theorem 8.4.6, \(g_n'(x) = \int_c^d f_x(x,t) dt\). We have \((g_n(x)) \to F(x)\) pointwise, and \((g_n')\) converges uniformly to \(\int_c^\infty f_x(x,t) dt\). Therefore by the Differentiable Limit Theorem (Theorem 6.3.1),
\[F'(x) = \int_c^\infty f_x(x,t) dt\]
\end{solution}
