\section{Inventing the Factorial Function}
\begin{exercise}
For \(n \in N\), let
\[N\# = n + (n-1) + (n-1) + \cdots + 2 + 1\]
\enum{
\item Without looking ahead, decide if there is a natural way to define \(0\#\). How about \((-2)\#\)? Conjecture a reasonable value for \(\frac{7}{2}\#\).
\item Now prove \(n\# = \frac{1}{2}n (n+1)\) for all \(n \in \mathbf{N}\), and revisit part (a).
}
\end{exercise}
\begin{solution}
\enum{
\item Noting that \(N\# = (N-1)\# + N\) and \(1\# = 1\), we could have \(0\# = 0\), \(-1\# = 0\), and \(-2\# = 1\). \(7/2\#\) could just be defined to be the result from linearly interpolating between \(3\# = 6\) and \(4\# = 10\) to get \(8\).
\item This is obviously true for \(n = 1\), and
\[(n+1)\# = n+1+n\# = n+1 + \frac{1}{2}n(n+1) = (n+1)\left(1 + \frac{n}{2}\right) = (n+1)\left(\frac{n+2}{2}\right)\]
which proves the formula by induction.
}
\end{solution}
\begin{exercise}
Verify that the series converges absolutely for all \(x \in \mathbf{R}\), that \(E(x)\) is differentiable on \(\mathbf{R}\), and \(E'(x) = E(x)\).
\end{exercise}
\begin{solution}
Note that
\[\sum^\infty_{n=0} \abs{\frac{x^n}{n!}} = E(\abs{x})\]
so we only need to show the series converges for \(x \geq 0\).

Fix \(x \geq 0\) and let \(N > x\) for \(N \in \mathbf{N}\). We have
\[E(x) = \sum^{2N}_{n=0} \frac{x^n}{n!} + \sum_{n=2N+1}^\infty \frac{x^n}{n!} \leq K + \sum_{n=2N+1}^\infty \frac{N^n}{N!(N+1)^n} = K + \frac{1}{N!}\sum_{n=2N+1}^\infty \left(\frac{N}{N+1}\right)^n\]
for some finite constant \(K\). The infinite series left over is a geometric series which converges.

Term-by-term differentiation is safe to apply on power series which converge (Theorem 6.5.6), and it's clear that \(E'(x) = E(x)\) when applying termwise differentiation.
\end{solution}

\begin{exercise}
\enum{
\item Use the results of Exercise 2.8.7 and the binomial formula to show that \(E(x+y) = E(x)E(y)\) for all \(x,y\in \mathbf{R}\).
\item Show that \(E(0) = 1\) , \(E(-x) = 1/E(x)\), and \(E(x) > 0\) for all \(x \in \mathbf{R}\).
}
\end{exercise}
\begin{solution}
\enum{
\item
\[\begin{aligned}
    E(x+y) &= \sum^\infty_{n=0} \frac{(x+y)^n}{n!} = \sum^\infty_{n=0} \frac{1}{n!} \sum^n_{i=0} \frac{n!}{i!(n-i)!} x^i y^{n-i} = \sum^\infty_{n=0} \sum^n_{i=0} \left(\frac{x^i}{i!}\right) \left(\frac{y^{n-i}}{(n-i)!}\right) \\
    &= \sum^\infty_{i=0} \sum^\infty_{j=0} \left(\frac{x^i}{i!}\right) \left(\frac{y^j}{j!}\right) = \left(\sum^\infty_{i=0} \frac{x^i}{i!}\right)\left(\sum^\infty_{i=0} \frac{y^i}{i!}\right) = E(x) E(y)
\end{aligned}
    \]
\item For \(E(0)\), all terms for \(n \geq 1\) become 0, so \(E(0) = 1\). We showed somewhat informally in Exercise 6.6.5(c) that \(E(-x) E(x) = 1\) by collecting common terms. However, Exercise 2.8.7 lets us conclude that \(E(-x)E(x)\) does in fact equal \(\sum^\infty_{n=0} d_n\) where \(d_n\) is defined as in Exercise 6.6.5(c). Finally, it's clear that \(E(x) > 0\)  for \(x \geq 0\) simply because all terms are positive; then \(E(-x) = 1/E(x) > 0\) and so \(E(x) > 0\) for \(x < 0\) as well.
}
\end{solution}

\begin{exercise}
Define \(e = E(1)\). Show \(E(n) = e^n\) and \(E(m/n) = \left(\sqrt[n]{e}\right)^m\) for all \(m,n \in \mathbf{Z}\).
\end{exercise}
\begin{solution}
We have for \(n \geq 1\) that
\[E(n) = E\left(\sum^n_{i=1} 1\right) = \prod^n_{i=1} E(1) = e^n \]
We also have for \(n=0\), \(E(0) = 1 = e^0\), by the standard definition of \(a^n\) for any \(a \in \mathbf{R}\). Finally for \(n > 0\), \(e^{-n} = 1/e^n = 1/E(n) = E(-n)\), so \(e^n = E(n)\) for all \(n \in \mathbf{Z}\).

By definition \(\sqrt[n]{e}\) is the unique positive number which satisfies \((\sqrt[n]{e})^n = e\). \(E(1/n)\) satisfies this equality, since
\[E(1) = E\left(\sum^n_{i=1} \frac{1}{n}\right)= \prod^n_{i=1} E(1/n) \]
so \(\sqrt[n]{e} = E(1/n)\). Finally
\[E(m/n) = E\left(\sum^m_{i=1} \frac{1}{n}\right) = \prod^m_{i=1}E(1/n) = \left(\sqrt[n]{e}\right)^m\]
\end{solution}

\begin{exercise}
Show \(\lim_{x \to \infty} x^n e^{-x} = 0\) for all \(n = 0,1, 2, \dots\).
\end{exercise}
\begin{solution}
Note that for a fixed \(n\), \(\forall K\in \mathbf{R}\),we can find \(N > 0\) so that whenever \(x \geq N\), \(e^x x^{-n} > K\). We do this by noting
\[\frac{E(x)}{x^n} > \frac{x^{n+1}}{(n+1)! x^n} = \frac{x}{(n+1)!}\]
and setting \(N = K(n+1)! \).

Now, let \(\epsilon > 0\), and find \(M\) so that \(x \geq M\) implies \(e^x x^{-n} > 1/\epsilon\). Then
\[x^n e^{-x} = \frac{1}{x^{-n}e^x} < \epsilon\]
as desired.
\end{solution}

\begin{exercise}
\enum{
\item Explain why we know \(e^x\) has an inverse function---let's call it \(\log x\)---defined on the strictly positive real numbers and satisfying
\enumr{
    \item \(\log(e^y) = y\) for all \(y \in \mathbf{R}\) and
    \item \(e^{\log x} = x\), for all \(x > 0\).
}
\item Prove \((\log x)' = 1/x.\) (See Exercise 5.2.12.)
\item Fix \(y > 0\) and differentiate \(\log(xy)\) with respect to \(x\). Conclude that
\[\log (xy) = \log x + \log y \text{\quad for all } x,y > 0.\]
\item For \(t > 0\) and \(n \in \mathbf{N}\), \(t^n\) has the usual interpretation as \(t \cdot t \cdots t\) (n times). Show that
\[t^n = e^{n \log t} \text{\quad for all } n \in \mathbf{N}\]
}
\end{exercise}
\begin{solution}
\enum{
\item Let \(y > x\), and let \(z = y-x > 0\). Noting that \(e^z > 1\) (as can be easily verified by looking at the definition of \(E\)), \(e^y = e^x e^z > e^x\) and therefore \(e^x\) is strictly increasing; therefore \(e^x\) does have an inverse function. In Exercise 8.4.5 we showed that \(e^x\) can achieve any value in \((0, \infty)\); hence \(\log x\) is defined for \(x > 0\). \(\log(e^y) = y\) and \(e^{\log x} = x\) stem from the definition of an inverse function.
\item For convenience of notation let \(f(x) = e^x\) and \(g(x) = f^{-1}(x) = \log x\). We have, from Exercise 5.2.12,
\[(\log x)' = g'(x) = \frac{1}{f'(g(x)} = \frac{1}{f(g(x)} = \frac{1}{e^{\log x}} = \frac{1}{x}\]
\item
\[(\log (xy))' = \frac{y}{xy} = \frac{1}{x} = (\log x)'\]
and therefore \(\log (xy) = \log x + C \) for some constant \(C\). Now
\[xy = e^{\log(xy)} = e^{\log x} e^C = x e^C \]
and therefore \(y = e^C\) and \(C = \log y\).
\item Combine \(t = e^{\log t}\) and \(\prod^n_{i=1} e^a = e^{na}\)
}
\end{solution}

\begin{exercise}
\enum{
\item Show \(t^{m/n}\) = \((\sqrt[n]{t})^m\) for all \(m, n \in \mathbf{N}\).
\item Show \(\log(t^x) = x \log t\), for all \(t > 0\) and \(x \in \mathbf{R}\).
\item Show \(t^x\) is differentiable on \(\mathbf{R}\) and find the derivative.
}
\end{exercise}
\begin{solution}
\enum{
\item The properties of \(e^x\) proved in Exercise 8.4.3 are trivially shown to be true of \(t^x\) as well. Then the same strategy as Exercise 8.4.4 can be taken, replacing \(e^x\) with \(t^x\).
\item Noting that \(t^x > 0\), take \(\log\) of both sides of the definition of \(t^x\).
\item
\[(t^x)' = \left(e^{x \log t}\right)' = \left(e^{x \log t}\right) \log t = t^x \log t\]
}
\end{solution}

\begin{exercise}
    Inspired by the fact that \(0! = 1\) and \(1! = 1\), let \(h(x)\) satisfy
\enumr{
\item \(h(x) = 1\) \quad for all \(0 \leq x \leq 1\), and
\item \(h(x) = x h(x-1) \) \quad for all \(x \in \mathbf{R}\).
}
\enum{
    \item Find a formula for \(h(x)\) on \([1,2]\), \([2,3]\), and \([n, n+1]\) for arbitrary \(n \in \mathbf{N}\).
    \item Now do the same for \([-1, 0]\), \([-2, -1]\), and \([-n, -n+1]\).
    \item Sketch \(h\) over the domain \([-4, 4]\).
}
\end{exercise}
\begin{solution}
\enum{
\item Over \([1,2]\), \(h(x) = x h(x-1) = x\). Over \([2,3]\), \(h(x) = x h(x-1) = x(x-1)\). Over \([n, n+1]\), \(h(x) = \prod^{n-1}_{i=0} x-i\), which can readily be proven with induction.
\item Over \([-1, 0]\), \((x+1) h(x) = h(x+1) = 1 \implies h(x) = \frac{1}{x + 1}\). Over \([-2, 1]\), \((x+1) h(x) = h(x+1) = \frac{1}{(x+1)} \implies h(x) = \frac{1}{(x+1)^2}\). Over \([-n, -n+1]\), \(h(x) = (x+1)^{-n}\).
\item This is an exercise best left for your favourite graphing utility.
}
\end{solution}

\begin{exercise}
\enum{
\item Show that the improper integral \(\int_a^\infty f\) converges if and only if, for all \(\epsilon > 0\) there exists \(M > a\) such that whenever \(d > c \geq M\) it follows that
\[\abs{\int_c^d f} < \epsilon.\]
(In one direction it will be useful to consider the sequence \(a_n =  \int_a^{a+n} f\).)
\item Show that if \(0 \leq f \leq g\) and \(\int_a^\infty g\) converges then \(\int_a^\infty f\) converges.
\item Part (a) is a Cauchy criterion, and part (b) is a comparison test. State and prove an absolute convergence test for improper integrals.
}
\end{exercise}
\begin{solution}
\enum{
\item
In the forward direction, assume that \(\lim_{b \to \infty}\int_a^b f\) exists. Note that for any \(c\),
\[\int_c^\infty f = \lim_{b \to \infty} \int_a^c f + \int_c^b f - \int_a^c f = \int_c^a f + \int_a^\infty f \]
Now, choose \(M\) large enough to ensure that for \(m \geq M\),
\[\abs{\int_a^\infty f - \int_a^m f} = \abs{\int_m^\infty f} < \frac{\epsilon}{2}\]
Now if \(d > c \geq M\),
\[\abs{\int_c^d f} = \abs{\int_c^\infty f - \int_d^\infty f} \leq \abs{\int_c^\infty f} + \abs{\int_d^\infty f} < 2 \frac{\epsilon}{2} = \epsilon\]

In the reverse direction, define \(M_n\) large enough so that \(d > c \geq M_n\) implies \(\abs{\int_c^d f} < 1/n \) and satisfying \(M_n > a + n\). Define the sequence \(x_n = \int_a^{M_n} f\), and note that \((x_n)\) is a Cauchy sequence and converges to some limit \(x\). Thus for any \(\epsilon > 0\), we can find \(N_1\) so that for \(n > N_1\),
\[\abs{x - \int_a^{M_n} f} < \frac{\epsilon}{2}\]
and set \(N > \max\{2/\epsilon, N_1 \}\) so that for \(b > c =M_{N} \),
\[\abs{\int_{M_N}^b f} = \abs{\int_a^b f - \int_a^{M_N} f} < \frac{\epsilon}{2}\]
and so \(b > M_N\) implies
\[\abs{\int_a^b - x} \leq \abs{x - \int_a^{M_N} f} + \abs{\int_a^{M_N} f - \int_a^b f} < \epsilon\]
showing that \(\int_a^\infty f = x\).
\item For any \(\epsilon > 0\), from part (a) and since \(\int_a^\infty g\) converges, \(\exists M\) so that for \(d > c \geq M\),
\[\epsilon > \abs{\int_c^d g} = \int_c^d g \geq \int_c^d f = \abs{\int_c^d f}\]
implying \(\int_a^\infty f\) converges, where we have taken advantage of the fact that \(\int_c^d f \geq 0\) and \(\int_c^d g \geq 0\).

\item The test is that if \(\int_a^\infty |f|\) converges, then so does \(\int_a^\infty f\). To prove this, we use the same strategy as part (b): for any \(\epsilon > 0\), \(\exists M\) so that for \(d > c \geq M\),
\[\epsilon > \abs{\int_c^d \abs{f}} = \int_c^d \abs{f} \geq \abs{\int_c^d f}\]
implying \(\int_a^\infty f\) converges.

}
\end{solution}

\begin{exercise}
\enum{
\item Use the properties of \(e^t\) previously discussed to show
\[\int_0^\infty e^{-t} dt = 1.\]
\item Show
\[\frac{1}{\alpha} = \int_0^\infty e^{-\alpha t} dt, \text{\quad for all } \alpha > 0.\]
}
\end{exercise}
\begin{solution}
\enum{
\item The antiderivative of \(e^{-t}\) is \(-e^{-t}\), so
\[\lim_{b \to \infty} \int_0^b e^{-t} dt = \lim_{b \to \infty} = \lim_{b \to \infty} -e^{-b} + e^0 = 1\]
\item The antiderivative of \(e^{-\alpha t}\) is \(-e^{-\alpha t} / \alpha\), so
\[\lim_{b \to \infty} \int_0^b e^{-\alpha t} dt  = \lim_{b \to \infty} \frac{1}{\alpha }\left(1 -e^{-\alpha b}\right) = \frac{1}{\alpha}\]
}
\end{solution}

\begin{exercise}
\enum{
\item Evaluate \(\int_0^b t e^{-\alpha t}\) using the integration-by-parts formula from Exercise 7.5.6. The result will be an expression in \(\alpha\) and \(b\).
\item Now compute \(\int_0^\infty t e^{-\alpha t} dt \) and verify equation (4).
}
\end{exercise}
\begin{solution}
\enum{
\item
\[\int_0^b te^{-\alpha t} dt = -\frac{b e^{-\alpha b}}{\alpha} + \frac{1}{\alpha} \int_0^b e^{-\alpha t} =-\frac{b e^{-\alpha b}}{\alpha} + \frac{1}{\alpha^2} (1 - e^{-\alpha b})\]
\item \(\lim_{b \to \infty} -\frac{b e^{-\alpha b}}{\alpha} + \frac{1}{\alpha^2} (1 - e^{-\alpha b}) = \frac{1}{\alpha^2}\)
}
\end{solution}

\begin{exercise}
Assume the function \(f(x,t)\) is continuous on the rectangle \(D = \{(x,t):a \leq x \leq b,c \leq t \leq d\}\). Explain why the function
\[F(x) = \int_c^d f(x,t) dt\]
is properly defined for all \(x \in [a,b]\).
\end{exercise}
\begin{solution}
For a fixed \(x\), \(f(x,t)\) is continuous with respect to \(t\). To see this, we need to show \(\forall t_0 \in [c,d],\epsilon > 0\), we can find \(\delta\) so that \(\abs{t - t_0} < \delta \) implies \(\abs{f(x,t) - f(x, t_0)} < \epsilon\). But since
\[\abs{t - t_0} = \|(x,t) - (x, t_0) \|\]
we can just use the continuity of \(f\) over \(D\) to conclude \(f(x,t)\) is continuous with respect to \(t\), and therefore \(\int_c^d f(x,t) dt\) is properly defined.
\end{solution}

\begin{exercise}
Prove Theorem 8.4.5.
\end{exercise}
\begin{solution}
Since \(f(x,t)\) is uniformly continuous, for any \(\epsilon > 0\), we can find \(\delta\) so that \(\abs{f(x,t) - f(x_0, t)} < \frac{\epsilon}{d-c}\) whenever \(\|(x,t) - (x_0, t) \| = \abs{x - x_0} < \delta\). Then when \(\abs{x - x_0} < \delta\),
\[\abs{F(x) - F(x_0)} \leq \int_c^d \abs{f(x, t) - f(x_0, t)} dt < \int_c^d \frac{\epsilon}{d-c} dt = \epsilon \]
showing \(F(x)\) is continuous.
\end{solution}

\begin{exercise}
Finish the proof of Theorem 8.4.6.
\end{exercise}
\begin{solution}
    Note that
    \[\frac{F(z) - F(x)}{z-x} = \int_c^d \frac{f(z,t) - f(x,t)}{z-x} dt\]
so choosing \(\delta\) so that \(0 < |z-x| < \delta\) implies
\[\abs{f_x(x,t) - \frac{f(z,t) - f(x,t)}{z-x}} < \frac{\epsilon}{d-c}\]
we have
\[\abs{\frac{F(z) - F(x)}{z-x} - \int_c^d f_x(x,t)dt} = \abs{\int_c^d \frac{f(z,t) - f(x,t)}{z-x} - f_x(x,t)} < \int_c^d \abs{\frac{\epsilon}{d-c}} = \epsilon\]
as desired.
\end{solution}

\begin{exercise}
\enum{
\item Show that the improper integral \(\int_0^\infty e^{-xt} dt\) converges uniformly to \(1/x\) on the set \([1/2, \infty)\).
\item Is the convergence uniform on \(0, \infty\)?
}
\end{exercise}
\begin{solution}
\enum{
\item From our earlier work, we have that
\[\int_0^\infty e^{-xt} dt - \int_0^d e^{-xt} = \frac{e^{-xd}}{x} \leq 2e^{-d/2}\]
Notably this is not dependent on \(x\), and since \(\lim_{d \to \infty} 2e^{-d/2} = 0\), we have that \(\int_0^\infty e^{-xt} dt\) converges uniformly.
\item For any fixed \(d\), \[\lim_{x \to 0^+} \frac{e^{-xd}}{x} = \infty\]
implying that the convergence cannot be uniform on \((0, \infty)\).
}
\end{solution}

\begin{exercise}
Prove the following analogue of the Weierstrauss M-Test for improper integrals: If \(f(x,t)\) satisfies \(\abs{f(x,t)} \leq g(t)\) for all \(x \in A\) and \(\int_a^\infty g(t) dt\) converges, then \(\int_a^\infty f(x,t) dt\) converges uniformly on \(A\).
\end{exercise}
\begin{solution}
We want, \(\forall \epsilon> 0\), to find \(M > a\) so
\[\abs{\int_a^\infty f(x,t) dt - \int_a^b f(x,t) dt} <\epsilon\]
for all \(b > M\). But
\[\abs{\int_a^\infty f(x,t) dt - \int_a^b f(x,t) dt} = \abs{\int_b^\infty f(x,t) dt} \leq \int_a^\infty  \abs{f(x,t)} dt \leq \int_b^\infty g(t) dt \]
and since \(\int_a^\infty g(t)\) converges, \(\lim_{b \to \infty} \int_b^\infty g(t) dt  = 0\), which can easily be used to finish the proof.
\end{solution}

\begin{exercise}
Prove Theorem 8.4.8.
\end{exercise}
\begin{solution}
Let \(F_n(x) = \int_c^{c+n} f(x,t)dt\) and note that \(F_n(x)\) is continuous. So
\(F(x) = \lim_{n \to \infty} F_n(x)\) must also be continuous, and since \([a,b]\) is compact \(F(x)\) is also uniformly continuous over \([a,b]\).
\end{solution}

\begin{exercise}
    Prove Theorem 8.4.9.
\end{exercise}
\begin{solution}
Let \(g_n(x) = \int_c^{c+n} f(x,t) dt\). From Theorem 8.4.6, \(g_n'(x) = \int_c^d f_x(x,t) dt\). We have \((g_n(x)) \to F(x)\) pointwise, and \((g_n')\) converges uniformly to \(\int_c^\infty f_x(x,t) dt\). Therefore by the Differentiable Limit Theorem (Theorem 6.3.1),
\[F'(x) = \int_c^\infty f_x(x,t) dt\]
\end{solution}

\begin{exercise}
\enum{
\item Although we verified it directly, show how to use the theorems in this section to give a second justification for the formula
\[\frac{1}{\alpha^2} = \int_0^\infty te^{-\alpha t} dt, \text{\quad for all } \alpha > 0.\]
\item Now derive the formula
\[\frac{n!}{\alpha^{n+1}} = \int_0^\infty t^n e^{-\alpha t} dt, \text{\quad for all }\alpha > 0.\]
}
\end{exercise}
\begin{solution}
\enum{
\item We would like to use Theorem 8.4.9, with \(f(\alpha, t) = e^{-\alpha t}\) and \(F(\alpha) = \int_0^\infty f(\alpha, t) dt\). We already have that \(F(\alpha) = 1/\alpha\) exists for \(\alpha > 0\). So, we need to show that \(f(\alpha, t)\) and \(f_\alpha(\alpha, t) = -te^{\alpha t}\) are continuous and that \(\int_0^\infty f_\alpha(\alpha, t) dt\) converges uniformly.

Let's first show that \(e^{-\alpha t}\) is continuous.
For any fixed \(\alpha_0\) and \(t_0\), we want \(\forall \epsilon > 0\), that whenever \(\|(\alpha ,t) - (\alpha_0, t_0)\| < \delta > 0\), that
\[\abs{e^{-\alpha t} - e^{-\alpha_0 t_0}} < \epsilon\]
Since \(e^{-x}\) is continuous, we can find \(\delta_1\) so that
\[\abs{\alpha_0 t_0 - at} < \delta_1 \implies \abs{e^{-\alpha_0 t_0} - e^{-at}} < \epsilon \]
Now if \(\abs{t_0 - t} < \frac{\delta_1}{2 \abs{\alpha_0}} = \delta_t\), \(\abs{t_0 - t} < 1\), and \(\abs{\alpha_0 - \alpha} < \frac{\delta_1}{2 (\abs{t_0} + 1)} = \delta_\alpha\), then
\[\abs{\alpha_0 t_0 - \alpha t} \leq \abs{\alpha_0} \abs{t_0 - t} + \abs{t}\abs{\alpha_0 - a} < \delta_1\]
We can ensure both of these conditions by setting \(\delta = \min \{\delta_t, \delta_\alpha, 1\}\), thus showing \(f(\alpha, t)\) is continuous.

It's fairly easy to show that if \(g(x)\) is continuous, so is \(g(x,y) = g(x)\). Additionally, a careful re-reading of the proofs for Theorem 4.2.3 (Sequential Criterion for Functional Limits) and Exercise 4.2.1 (Algebraic Limit Theorem for Functional Limits) show that they can be reused for complete metric spaces such as \(\mathbf{R}^2\) with the usual metric. Thus Theorem 4.3.4 (Algebraic Continuity Theorem) can also be generalized to \(\mathbf{R}^2\), and we can conclude for any \(x \in \mathbf{R}\), the function \(g(\alpha, t) = t^x e^{-\alpha t}\) is continuous. Thus, both \(f\) and \(f_\alpha\) are continuous.

Now we need to show \(\int_0^\infty f_\alpha(\alpha, t) dt\) converges uniformly when \(\alpha \geq p > 0\). To make part (b) easier we'll prove the more general statement that \(\int_0^\infty t^x e^{-\alpha t} dt\) converges uniformly, for any \(x \in \mathbf{R}\) with \(x \geq 0\). As a prerequisite, note that if \(\int_a^\infty g(x, y) dy\) converges uniformly, then so does \(\int_b^\infty g(x,y) dy\), provided \(\int_a^b g(x,y) dy\) is well defined.

We can adapt the solution to Exercise 8.4.5 to show that for any \(x \in \mathbf{R}\), \(\alpha > 0\), \(\lim_{t \to \infty} t^x e^{-\alpha t} = 0\). Thus, for any fixed \(x \geq 0\), we can find some \(M\) so that
\(g(\alpha) = \int_M^\infty t^x e^{-\alpha t} dt\)
converges uniformly, by Exercise 8.4.16 (variant of the Weierstrauss M-Test) and comparison against \(e^{-\alpha t /2}\). This in turn shows that \(\int_0^\infty t^x e^{-\alpha t} dt\) converges uniformly.

Finally, before applying Theorem 8.4.9 to get our desired result, we need to define the domain \(D\). For a given \(\alpha > 0\), we want \(D\) to contain all points of the form \(\alpha, t\), so define \(D = {(x,t): \alpha / 2 \leq x \leq \alpha + 1, 0 \leq t}\).

\item This is just a repeated application of Theorem 8.4.9 on the formula proved in part (a); the solution to part (a) shows that all of the prerequisites to Theorem 8.4.9 are always met in this process.
}
\end{solution}

\begin{exercise}
\enum{
\item Show that \(x!\) is an infinitely differentiable function on \((0, \infty)\) and produce a formula for the \(n^{th}\) derivative. In particular show that \((x!)'' > 0\).
\item Use the integration-by-parts formula employed earlier to show that \(x!\) satisfies the functional equation
\[(x+1)! = (x+1)x!\]
}
\end{exercise}
\begin{solution}
\enum{
\item Before continuing, note that \(t^x\) currently isn't defined at \(t=0\); this can easily be solved by defining \(0^x = 0\).

We show that \(x!\) is infinitely differentiable at \(x_0 > 0\) by repeatedly applying Theorem 8.4.9, over the domain \(D = \{(x,t) : x_0 / 2 \leq x \leq x_0 + 1, 0 \leq t\}\). Denote the \(n\)'th derivative of \(t^x e^{-t}\) with respect to \(x\) as \(f_n(x,t) = t^x e^{-t} \log^n t\) (except at \(t = 0\), where \(f(x, t) = 0\)). The work in Exercise 8.4.19 (a) means that to show \(f_n\) is continuous over \(D\), we only need to show that \(t^x\) is continuous.
% We can use the same technique as was used to show \(g(\alpha, t) = e^{-\alpha t}\) is continuous in Exercise 8.4.19 (a).
Note that for any fixed \((x_1, t_1) \in D\),
\[ \abs{t_1^{x_1}- t^x } \leq \abs{t_1^{x_1} - t^{x_1}} + \abs{t^x}\abs{t^{x_1 - x} - 1}\]
Since \(g(t) = t^{x_1}\) is continuous, we can require \(\abs{t_1 - t} < \delta_t\) to ensure the first term is less than \(\epsilon/2\). Since \(t^x\) is bounded for finite \(t\) and \(x\), let \(M_1\) be an upper bound for \(|t^x|\) over \(V_1(x_1, t_1) \cap D\). Let \(M_2\) be an upper bound for \(|\log t|\) over \(V_{t_1 / 2}(t_1)\).

We can find \(\delta_1\) so that when \(\abs{(x_1 - x) \log t} < \delta_1\),
\(\abs{e^{(x_1 - x) \log t} - 1} < \frac{\epsilon}{2 M_1}\). Then requriring \(|x_1 - x| < \frac{\delta_1}{M_2} = \delta_x\) guarentees that
\[\abs{t^x}\abs{t^{x_1 - x} - 1} \leq M_1 \abs{e^{(x_1 - x) \log t} - 1} < \frac{\epsilon}{2}\]
Putting it all together,
\[\|(x, t) - (x_0, t_0)\| < \min\left\{\delta_t, \delta_x, 1, \frac{t_1}{2}\right\} \implies \abs{t^x - t_0^{x_0}} < \epsilon \]
proving \(t^x\) is continuous.

We also need to show that \(\int_0^\infty f_n(x,t) dt\) converges uniformly. Now, since \(x > \log x \geq 0\) for \(x \geq 1\), we can show \(\int_1^\infty f_n(x,t) dt\) converges uniformly by comparison against \(\int_1^\infty t^{x + n} e^{-t} dt\). We also have \(f_n(x,t)\) continuous in \(t\) over \([0, 1]\) so \(\int_0^1 f_n(x,t) dt\) is well defined.

Thus, repeatedly applying Theorem 8.4.9 lets us conclude that
\[(x!)^{(n)} = \int_0^\infty t^x e^{-t} \left(\log t\right)^n dt\]
Note that for \((x!)''\), the term under the integral is always positive, so \((x!)'' > 0\).

\item
\[x! = \int_0^\infty t^x e^{-t} dt = 0^x e^0 - \lim_{t \to \infty} t^x e^{-t} + \int_0^\infty x t^{x-1} e^t dt = x (x-1)! \]
which is equivalent to saying \((x+1)! = (x+1) x!\).
}
\end{solution}

\begin{exercise}
\enum{
\item Use the convexity of \(\log(f(x))\) and the three intervals \([n-1,n]\), \([n, n+x]\), and \([n, n+1]\) to show
\[x \log(n) \leq \log (f(n+x)) - \log (n!) \leq x \log (n+1)\]
\item Show \(\log(f(n+x)) = \log (f(x)) + \log((x+1)(x+2) \cdots (x+n))\).
\item Now establish that
\[0 \leq \log(f(x)) - \log\left(\frac{n^x n!}{(x+1)(x+2)\cdots(x+n)}\right) \leq x \log (1 + \frac{1}{n})\]
\item Conclude that
\[f(x) = \lim_{n \to \infty} \frac{n^x n!}{(x+1)(x+2) \cdots (x+n)}, \text{\quad for all } x \in (0, 1].\]
\item Finally, show that the conclusion in \(d\) holds for all \(x \geq 0 \).
}
\end{exercise}

\begin{solution}
\enum{
\item Since the slopes are increasing between these three intervals,
\[\log(f(n)) - \log(f(n-1)) \leq \frac{\log(f(n+x)) - \log(f(n))}{x} \leq \log(f(n+1)) - \log(f(n)) \]
\[x \log\left(\frac{f(n)}{f(n-1)}\right) \leq \log(f(n+x)) - \log(n!) \leq x \log\left(\frac{f(n+1)}{f(n)}\right) \]
\[x \log(n) \leq \log(f(n+x)) - \log(n!) \leq x \log (n+1)\]
\item By property (ii),
\[f(n+x) = f(x) \prod^n_{i=1} (x+i) \]
so
\[\log(f(n+x)) = \log(f(x)) + \log\left(\prod^n_{i=1}(x+i)\right) \]
\item Plug (b) into (a):
\[x \log(n) \leq \log(f(x)) - \left(\log(n!) - \log\left(\prod^n_{i=1}(x+i)\right) \right) \leq x \log (n+1)\]
\[0 \leq \log(f(x)) - \left(\log(n!) - \log\left(\prod^n_{i=1}(x+i)\right)  + \log\left(n^x\right)\right) \leq x \log (n+1) - x \log(n)\]
\[0 \leq
\log(f(x)) - \log\left(\frac{n^x n!}{\prod^n_{i=1}(x+i)}\right)
\leq x \log \left(\frac{n+1}{n}\right) = x \log \left(n + \frac{1}{n}\right)\]
}

\item \(\lim_{n \to \infty} \log \left(1 + \frac{1}{n}\right) = 0\) so by the Squeeze Theorem,
\[\log(f(x)) - \lim_{n \to \infty} \log\left(\frac{n^x n!}{\prod^n_{i=1}(x+i)}\right) =0
\]
and since \(\log\) is continuous,
\[f(x) = \lim_{n \to \infty} \frac{n^x n!}{\prod^n_{i=1}(x+i)}\]

\item We only need to show this equation holds for \(x = 0\). We have \(f(0) = 1\) by property (i), and
\[\lim_{n \to \infty} \frac{n^0 n!}{\prod^{n}_{i=1} i} = \lim_{n \to \infty} \frac{n!}{n!} = 1 = f(0)\]
\end{solution}

\begin{exercise}
\enum{
\item Where does \(g(x) = \frac{x}{x! (-x)!}\) equal zero? What other familiar function has the same set of roots?
\item The function \(e^{-x^2}\) provides the raw material for the all-important Gaussian bell curve from probability, where it is known that \(\int_{-\infty}^\infty e^{-x^2} dx = \sqrt{\pi}\). Use this fact (and some standard integration techniques) to evaluate \((1/2)!\).
\item Now use (a) and (b) to conjecture a striking relationship between the factorial function and a well-known function from trigonometry.
}
\end{exercise}
\begin{solution}
\item \(g(x)\) has roots at all integers, as does \(\sin \pi x\).
\item Apply integration by parts:
\[\left(\frac{1}{2}\right)! = \int_0^\infty \sqrt(t) e^{-t} dt = \left(-\lim_{t \to \infty} \sqrt{t} e^{-t}\right) - (-\sqrt{0} e^{0}) + \int_0^\infty e^{-t} \frac{1}{2\sqrt{t}} dt = \int_0^\infty e^{-t} \frac{1}{2 \sqrt{t}} dt \]
Apply change-of-variable formula (Theorem 8.1.10), with \(u = \sqrt{t}\) taking advantage of \(t \geq 0\):
\[\int_0^\infty e^{-t} \frac{1}{2\sqrt{t}}dt = \int_0^\infty e^{-u^2} du = \frac{1}{2}\int_{-\infty}^\infty e^{-u^2} du = \frac{\sqrt{\pi}}{2}\]
\item We have \((-1/2)! = 2 (1/2)! = \sqrt{\pi}\), so from part (a) \(g(1/2) = \frac{1}{\pi}\). The conjecture is
\[\frac{x}{x! (-x)!} = \frac{\sin(\pi x)}{\pi}\]
\end{solution}

\begin{exercise}
As a parting shot, use the value for \((1/2)!\) and the Gauss product formula in equation (9) to derive the famous product formula for \(\pi\) discovered by John Wallis in the 1650s:
\[\frac{\pi}{2} = \lim_{n \to \infty} \left(\frac{2 \cdot 2}{1 \cdot 3}\right) \left(\frac{4 \cdot 4}{3 \cdot 5}\right) \frac{6 \cdot 6 }{5 \cdot 7} \cdots \left(\frac{2n \cdot 2n}{(2n-1)(2n+1)}\right)\]
\end{exercise}
\begin{solution}
\[\frac{\sqrt{\pi}}{2} = \lim_{n \to \infty} \sqrt{n} \prod^n_{i=1} \frac{i}{i + 1/2} = \lim_{n \to \infty} \sqrt{n} \prod^n_{i=1} \frac{2i}{2i + 1}\]
\[ \begin{aligned}
    \frac{\pi}{2} &= \lim_{n \to \infty} 2n \left(\prod^n_{i = 1} \frac{2i}{2i + 1}\right)\left(\prod^n_{i = 1} \frac{2i}{2i + 1}\right) = \lim_{n \to \infty} \left(\prod^n_{i = 1} \frac{2i}{2i - 1}\right)\left(\prod^n_{i = 1} \frac{2i}{2i + 1}\right)  \\
&= \lim_{n \to \infty} \prod^n_{i=1} \frac{(2i)^2}{(2i-1)(2i+1)}
\end{aligned}
\]
\end{solution}
